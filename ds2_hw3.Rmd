---
title: "ds2_hw3"
author: "Ruihan Zhang"
date: "2023-03-21"
output: pdf_document
---
```{r}
library(caret)
library(knitr)
library(glmnet)
library(mlbench)
library(splines)
library(mgcv)
library(pROC)
library(MASS)
library(earth)
library(tidyverse)
library(ggplot2)
library(pdp)
library(vip)
library(klaR)
library(AppliedPredictiveModeling)
```


```{r}
auto=read_csv("./auto.csv") %>% 
  janitor::clean_names() %>% 
  na.omit() %>% 
  mutate(mpg_cat = as.factor(mpg_cat),
         year = as.factor(year),
         origin = as.factor(origin),
         mpg_cat = fct_relevel(mpg_cat, c("low", "high")))
```
```{r}  
set.seed(2023)
train_index=createDataPartition(y=auto$mpg_cat, p=0.7, list = FALSE)
train=auto[train_index,]
test=auto[-train_index,]
#training data
x1=model.matrix(mpg_cat~.,train)[,-1]
y1=train$mpg_cat
#testing data
x2=model.matrix(mpg_cat~.,test)[,-1]
y2=auto$mpg_cat[-train_index]
```

```{r}
#1.
glm_fit=glm(mpg_cat~., data = auto, subset = train_index, family =binomial(link = "logit"))
summary(glm_fit)
```

The predictors weight, year79, year80, year81 and year82 are statistically significant, because the p-value of these predictors are smaller than 0.05

```{r}
pred_prob=predict(glm_fit, newdata = auto[-train_index,],
                           type = "response")
pred=rep("low", length(pred_prob))
pred[pred_prob>0.5]="high"
cm=confusionMatrix(data = factor(pred, levels = c("low","high")),
                reference = auto$mpg_cat[-train_index],
                positive = "high")
cm
cm$byClass["Balanced Accuracy"]
```
From the confusion matrix, the accuracy can be calculated by (52+53)/(52+53+5+6)=0.90517. The accuracy is close to 1, so the model works well. TN is 52. TP is 53. FN is 5. FP is 6. The 95% CI is (0.8367, 0.9517). The no information rate is 0.5. The p-value is less than 2e-16, which is pretty small, so the null hypothesis is rejected. Therefore, our model is significant. The kappa value is 0.8103, which is close to 1, so the model works well. Both sensitivity and specificity is close to 1. 
```{r}
#caret
ctrl=trainControl(method = "cv",
                     summaryFunction = twoClassSummary,
                     classProbs = TRUE)
glm_model=train(x1,y1,,method="glm",metric="ROC", trControl=ctrl)
summary(glm_model)
```


```{r,warning=FALSE,message=FALSE}
#2.
set.seed(2023)

mars.fit=train(auto[train_index,1:7],
                    auto$mpg_cat[train_index],
                    method = "earth",
                    tuneGrid = expand.grid(degree = 1:5, 
                                           nprune = 2:30),
                    metric = "ROC",
                    trControl = ctrl)
summary(mars.fit)
plot(mars.fit)
kable(mars.fit$bestTune,"simple")
coef(mars.fit$finalModel)
vip(mars.fit$finalModel)
```
When the degree is 2 with 9 terms, the AUC is the highest. 
```{r}
#3.
set.seed(2023)
lda_fit=lda(mpg_cat~., data=auto, subset=train_index)
plot(lda_fit)
lda_fit$scaling
```

```{r}
lda.pred=predict(lda_fit,newdata = test)
head(lda.pred$posterior)
```

```{r}
#use caret
set.seed(2023)
lda_fit2=train(mpg_cat~., data=train,method="lda",metric="ROC",trControl=ctrl)
lda_fit2$results
coef(lda_fit2$finalModel)
```

```{r}
#4.
set.seed(2023)
res=resamples(list(GLM=glm_model,MARS=mars.fit,LDA=lda_fit2))
summary(res)

```

```{r}
bwplot(res,metric = "ROC")
```

Based on the graph, the LDA model has the highest AUC. Thus, I choose the LDA model to predict the response. 
```{r}
lda_pred=predict(lda_fit2, newdata = test,type = "prob")[,2]
lda_roc=roc(test$mpg_cat, lda_pred)
lda_auc=lda_roc$auc[1]
modelName=c("LDA")
ggroc(list(lda_roc), legacy.axes = TRUE) + scale_color_discrete(labels=paste0(modelName,"(", round(lda_auc,3),")"),name="Models(AUC)")+geom_abline()
```
The AUC is 0.928. 

```{r}
test_pred_lda=predict(lda_fit2,newdata = test,type = "prob")
test_pred_prob=predict(glm_fit,newdata = test, type = "response")
pred2=rep("low",length(lda_pred))
pred2[lda_pred>0.5]="high"
confusionMatrix(data = as.factor(pred2),reference=test$mpg_cat,positive="high")
```

The misclassification rate of the LDA model is 1-0.8793=0.1207.

